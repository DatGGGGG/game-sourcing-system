{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "671de580-c4dc-4462-b34d-5d5e916af50c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import tiktoken\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from jsonschema import validate, ValidationError\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "from typing import List, Any, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d137f-d3ec-40d6-a8a7-687e7977f714",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38f7e8e-f5d4-4e39-9c8a-1e22a08d4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "CSV_PATH = \"output/concise_reviews.csv\"  # Path to your input CSV\n",
    "OUTPUT_DIR = \"output\"  # Output directory for JSON files\n",
    "OUTPUT_DIR_ITER_1 = \"output/iter_1\"\n",
    "OUTPUT_DIR_ITER_2 = \"output/iter_2\"\n",
    "OUTPUT_DIR_ITER_3 = \"output/iter_3\"\n",
    "OUTPUT_DIR_ITER_4 = \"output/iter_4\"\n",
    "OUTPUT_DIR_ITER_5 = \"output/iter_5\"\n",
    "OUTPUT_DIR_ITER_6 = \"output/iter_6\"\n",
    "OUTPUT_DIR_ITER_7 = \"output/iter_7\"\n",
    "OUTPUT_DIR_ITER_8 = \"output/iter_8\"\n",
    "OUTPUT_DIR_ITER_9 = \"output/iter_9\"\n",
    "OUTPUT_FILE = \"all_analysis_results.json\"  # Single output file\n",
    "GAME_URL = \"https://www.taptap.cn/app/209601?os=android\"\n",
    "\n",
    "# --- ChatTunables (adjust if you like) ---\n",
    "MODEL = \"gpt-4o\"\n",
    "MODEL_CONTEXT = 128_000          # gpt-4o context tokens (safe default)\n",
    "OUTPUT_BUFFER = 1_200            # tokens reserved for model output\n",
    "SAFETY_MARGIN = 1_000            # overhead for roles/formatting/etc.\n",
    "CHUNK_OUTPUT_MAX = 800           # max tokens per chunk summary\n",
    "REDUCE_OUTPUT_MAX = 1_200        # max tokens for the final merged summary\n",
    "TEMP = 0.2\n",
    "RETRY = 3\n",
    "RETRY_SLEEP = 2.0\n",
    "MODEL = \"gpt-4o\"\n",
    "MAX_TOKENS_PER_BATCH = 120000\n",
    "OUTPUT_TOKEN_BUFFER_PER_REVIEW = 1200\n",
    "API_KEY = \"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\"  # <-- Replace with your actual API key\n",
    "\n",
    "# === READING RAW TEXT OUTPUT FILES ===\n",
    "RAW_PREFIX = \"batch_\"\n",
    "RAW_SUFFIX = \"_raw.txt\"\n",
    "OUTPUT_JSON = \"all_reviews_compiled.json\"\n",
    "OUTPUT_CSV = \"all_processed_reviews_compiled.csv\"\n",
    "output_path = os.path.join(OUTPUT_DIR, OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "099622aa-bf44-4616-acee-73b8049c892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Schema definition (abbreviated - extend this as needed)\n",
    "\n",
    "review_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"q0\": {\"type\": \"integer\"},\n",
    "\n",
    "        \"q1\": {\"type\": [\"boolean\", \"string\"], \"enum\": [True, False, \"N/A\"]},\n",
    "        \"q2\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q3\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "        \"q4\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q5\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q6\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "        \"q7\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q8\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q9\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q10\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q11\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q12\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q13\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q14\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q15\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "        \"q16\": {\"type\": [\"boolean\", \"string\"], \"enum\": [True, False, \"N/A\"]},\n",
    "        \"q17\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q18\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "        \"q19\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q20\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q21\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q22\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q23\": {\"type\": [\"boolean\", \"string\"], \"enum\": [True, False, \"N/A\"]},\n",
    "        \"q24\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "        \"q25\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q26\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q27\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q28\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "        \"q29\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q30\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "        \"q31\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q32\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q33\": {\"type\": [\"boolean\", \"string\"], \"enum\": [True, False, \"N/A\"]},\n",
    "        \"q34\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "        \"q35\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q36\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q37\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q38\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "        \"q39\": {\"type\": [\"integer\", \"string\"], \"enum\": [-2, -1, 0, 1, 2, \"N/A\"]},\n",
    "        \"q40\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "\n",
    "        \"q41\": {\"type\": [\"string\"], \"minLength\": 1},\n",
    "    },\n",
    "    \"additionalProperties\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81529f75-4e94-4f2b-ac54-64f492e37f38",
   "metadata": {},
   "source": [
    "# Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42ea64bd-b2b1-4fcf-b76a-5ac7a16fbefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_from_list_of_dicts(list_of_dicts, fields_to_extract):\n",
    "    \"\"\"\n",
    "    Extracts only the specified fields from a list of dictionaries.\n",
    "\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {field: dicti.get(field, None) for field in fields_to_extract}\n",
    "        for dicti in list_of_dicts\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4eaf7b4-daa6-4995-a334-3efed728cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_keys_by_value(d, target_value):\n",
    "    \"\"\"\n",
    "    Returns a list of keys from dictionary `d` where the value equals `target_value`.\n",
    "\n",
    "    Args:\n",
    "        d (dict): Dictionary to search.\n",
    "        target_value: The value to match.\n",
    "\n",
    "    Returns:\n",
    "        list: List of keys whose value matches `target_value`.\n",
    "    \"\"\"\n",
    "    return [k for k, v in d.items() if v == target_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9b73651-3906-44db-8686-cdbd9782d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Estimate token count for a single user message (chat-style) for OpenAI's Chat API.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text (e.g. a review).\n",
    "        model (str): Model name (e.g. \"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo\").\n",
    "\n",
    "    Returns:\n",
    "        int: Estimated number of tokens used.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wrap the text as a chat message\n",
    "    messages = [{\"role\": \"user\", \"content\": text}]\n",
    "\n",
    "    # Get the encoding for the model\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Define per-message overheads\n",
    "    if model in {\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4o\"}:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Token counting not supported for model: {model}\")\n",
    "\n",
    "    total_tokens = 0\n",
    "    for message in messages:\n",
    "        total_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            total_tokens += len(encoding.encode(str(value)))\n",
    "            if key == \"name\":\n",
    "                total_tokens += tokens_per_name\n",
    "\n",
    "    total_tokens += 3  # Priming reply from assistant\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "452d0bea-4808-4e7c-abb1-a0373d865567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_count_tokens(reviews, review_id_field_name = \"review_id\"):\n",
    "    \"\"\"\n",
    "    Converts each review into a JSON-formatted string (including None values),\n",
    "    and computes token usage using the global count_tokens function.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: List of dicts with 'review_id' and 'token_count'.\n",
    "    \"\"\"\n",
    "    token_summaries = []\n",
    "\n",
    "    for review in reviews:\n",
    "        review_id = review.get(review_id_field_name, \"unknown\")\n",
    "\n",
    "        # Build JSON-like string\n",
    "        lines = []\n",
    "        for idx, (key, value) in enumerate(review.items()):\n",
    "            value_str = json.dumps(value)  # Properly quotes strings, keeps None as null, etc.\n",
    "            comma = \",\" if idx < len(review) - 1 else \"\"\n",
    "            lines.append(f'  \"{key}\": {value_str}{comma}')\n",
    "\n",
    "        review_string = \"{\\n\" + \"\\n\".join(lines) + \"\\n}\"\n",
    "        token_count = count_tokens(review_string)\n",
    "\n",
    "        # print(review_string)\n",
    "\n",
    "        token_summaries.append({\n",
    "            \"review_id\": review_id,\n",
    "            \"token_count\": token_count\n",
    "        })\n",
    "\n",
    "    return token_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81da32f4-f21e-4007-b69b-0974f4b23289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_reviews(reviews, max_tokens=MAX_TOKENS_PER_BATCH, output_token_buffer_per_review=OUTPUT_TOKEN_BUFFER_PER_REVIEW):\n",
    "    \"\"\"\n",
    "    Groups reviews into batches such that each batch's total estimated token usage\n",
    "    (input prompt + per-review input + per-review output buffer) stays within max_tokens.\n",
    "\n",
    "    Args:\n",
    "        reviews (list[dict]): List of review input dictionaries.\n",
    "        max_tokens (int): Maximum total tokens allowed per batch.\n",
    "        output_token_buffer_per_review (int): Tokens reserved for each review's output.\n",
    "\n",
    "    Returns:\n",
    "        list[list[dict]]: Batches of reviews.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = count_tokens(PROMPT_PREFIX)\n",
    "\n",
    "    for review in reviews:\n",
    "        review_text = f\"{review}\"\n",
    "        input_tokens = count_tokens(review_text)\n",
    "        total_estimated_tokens = input_tokens + output_token_buffer_per_review\n",
    "\n",
    "        if current_tokens + total_estimated_tokens > max_tokens and current_batch:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = [review]\n",
    "            current_tokens = count_tokens(PROMPT_PREFIX) + total_estimated_tokens\n",
    "        else:\n",
    "            current_batch.append(review)\n",
    "            current_tokens += total_estimated_tokens\n",
    "\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f9ce875-908d-46b7-b718-33615c7f1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_keys(data, key_mapping):\n",
    "    \"\"\"\n",
    "    Remaps the keys in each dictionary in the data list based on key_mapping.\n",
    "\n",
    "    Args:\n",
    "        data (list[dict]): Original list of dictionaries.\n",
    "        key_mapping (dict): Dictionary of old_key -> new_key.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: New list with remapped keys.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {key_mapping.get(k, k): v for k, v in entry.items()}\n",
    "        for entry in data\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b21fe9a-46f2-4c6b-9611-b01603643fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_column_values(df, column_name):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of value counts for a specific column in a DataFrame,\n",
    "    including NaN values represented as None.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame.\n",
    "        column_name (str): The column to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of {value: count}, including None for NaNs.\n",
    "    \"\"\"\n",
    "    counts = df[column_name].value_counts(dropna=False).to_dict()\n",
    "    \n",
    "    # Replace NaN key with None for clarity\n",
    "    cleaned_counts = {}\n",
    "    for key, value in counts.items():\n",
    "        if pd.isna(key):\n",
    "            cleaned_counts[None] = value\n",
    "        else:\n",
    "            cleaned_counts[key] = value\n",
    "\n",
    "    return cleaned_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84ea2c10-6099-4187-9c2b-2ecae208c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weighted_average(df, column_name):\n",
    "    \"\"\"\n",
    "    Calculates the weighted average from a {value: count} dictionary,\n",
    "    excluding the None key.\n",
    "\n",
    "    Args:\n",
    "        d (dict): Dictionary where keys are numeric values (or None),\n",
    "                  and values are their counts.\n",
    "\n",
    "    Returns:\n",
    "        float or None: The weighted average, or None if no valid data.\n",
    "    \"\"\"\n",
    "\n",
    "    d = count_column_values(df, column_name)\n",
    "    \n",
    "    total = 0\n",
    "    count = 0\n",
    "\n",
    "    for key, value in d.items():\n",
    "        if key is not None:\n",
    "            total += key * value\n",
    "            count += value\n",
    "\n",
    "    return total / count if count > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cff7193f-2ec1-4709-983b-721b697fb0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json(obj: Any) -> str:\n",
    "    \"\"\"Stable JSON encoding for prompts.\"\"\"\n",
    "    return json.dumps(obj, ensure_ascii=False, separators=(\",\", \":\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b5c14b4-196b-4821-b4af-63331dd72ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chunk_items_by_tokens(\n",
    "    items: List[Dict],\n",
    "    prompt: str,\n",
    "    model_context: int = MODEL_CONTEXT,\n",
    "    output_buffer: int = OUTPUT_BUFFER,\n",
    "    safety_margin: int = SAFETY_MARGIN,\n",
    ") -> List[List[Dict]]:\n",
    "    \"\"\"\n",
    "    Pack items into chunks so that:\n",
    "      tokens(system+user prompt + items JSON) + safety_margin + output_buffer <= model_context\n",
    "    \"\"\"\n",
    "    # Token budget for the *input side*\n",
    "    input_budget = model_context - output_buffer - safety_margin\n",
    "    if input_budget <= 0:\n",
    "        raise ValueError(\"Token budget is non-positive. Lower OUTPUT_BUFFER/SAFETY_MARGIN or increase context.\")\n",
    "\n",
    "    # Base tokens from messages without items\n",
    "    base_user_prefix = \"INPUT:\\n\"  # matches your format\n",
    "    base_tokens = count_tokens(prompt) + count_tokens(base_user_prefix)  # system + \"INPUT:\\n\"\n",
    "\n",
    "    batches: List[List[Dict]] = []\n",
    "    current: List[Dict] = []\n",
    "    current_tokens = base_tokens + count_tokens('{\"items\":[]}')  # minimal structure\n",
    "\n",
    "    for it in items:\n",
    "        # Estimate tokens if we add this item\n",
    "        item_json = _json(it)\n",
    "        # extra 1-2 chars for comma when appending; add small cushion\n",
    "        add_tokens = count_tokens(item_json) + 2\n",
    "\n",
    "        if current and (current_tokens + add_tokens) > input_budget:\n",
    "            batches.append(current)\n",
    "            current = [it]\n",
    "            current_tokens = base_tokens + count_tokens('{\"items\":[' + item_json + \"]}\") + 2\n",
    "        else:\n",
    "            current.append(it)\n",
    "            current_tokens += add_tokens\n",
    "\n",
    "    if current:\n",
    "        batches.append(current)\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f4a34ae-6ef6-4ebf-b341-786a5feadf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _call_response(model: str, system_prompt: str, user_payload: str, max_tokens: int) -> str:\n",
    "    \"\"\"Call Responses API with simple retries; return output_text.\"\"\"\n",
    "    for attempt in range(1, RETRY + 1):\n",
    "        try:\n",
    "            resp = client.responses.create(\n",
    "                model=model,\n",
    "                input=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                       {\"role\": \"user\", \"content\": user_payload}],\n",
    "                temperature=TEMP,\n",
    "                max_output_tokens=max_tokens,\n",
    "            )\n",
    "            return resp.output_text\n",
    "        except Exception as e:\n",
    "            if attempt == RETRY:\n",
    "                raise\n",
    "            time.sleep(RETRY_SLEEP * attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e5fb3f5-b797-45cd-8e90-d233a840f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_long_form_answers_with_ai(items: List[Dict], prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Summarize a potentially large list of items by:\n",
    "      1) chunking to fit the context window,\n",
    "      2) summarizing each chunk,\n",
    "      3) reducing the partial summaries into a final answer.\n",
    "\n",
    "    Returns the final synthesized text.\n",
    "    \"\"\"\n",
    "    # 1) Chunk\n",
    "    chunks = _chunk_items_by_tokens(items, prompt)\n",
    "    print(f\"⚙️  Synthesizing in {len(chunks)} chunk(s)...\")\n",
    "\n",
    "    # 2) Map: summarize each chunk\n",
    "    partial_summaries: List[str] = []\n",
    "    for idx, chunk in enumerate(chunks, 1):\n",
    "        user_content = \"INPUT:\\n\" + _json({\"items\": chunk})\n",
    "        summary = _call_response(MODEL, prompt, user_content, CHUNK_OUTPUT_MAX)\n",
    "        partial_summaries.append(summary)\n",
    "        print(f\"   ✅ Chunk {idx}/{len(chunks)} summarized (≈{count_tokens(summary)} tokens).\")\n",
    "\n",
    "    # If only one chunk, we’re done\n",
    "    if len(partial_summaries) == 1:\n",
    "        final = partial_summaries[0]\n",
    "        # Optional: print(final)\n",
    "        return final\n",
    "\n",
    "    # 3) Reduce: merge partial summaries\n",
    "    reduce_instructions = (\n",
    "        \"You will receive several partial summaries produced from different chunks of the same dataset. \"\n",
    "        \"Merge them into a single, cohesive answer that follows the **same instructions** as the original system prompt. \"\n",
    "        \"Avoid duplication; keep structure consistent; resolve conflicts sensibly. Be concise and complete.\"\n",
    "    )\n",
    "    reduce_user_content = \"PARTIAL_SUMMARIES:\\n\" + _json({\"summaries\": partial_summaries})\n",
    "\n",
    "    final_summary = _call_response(\n",
    "        MODEL,\n",
    "        system_prompt=f\"{prompt}\\n\\n[Reducer instructions]\\n{reduce_instructions}\",\n",
    "        user_payload=reduce_user_content,\n",
    "        max_tokens=REDUCE_OUTPUT_MAX,\n",
    "    )\n",
    "    # Optional: print(final_summary)\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384f4fe-7cb3-488f-a812-351fad1f76ee",
   "metadata": {},
   "source": [
    "# MAIN PROGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb0908d-8546-46dc-b40f-c2e169bf4d2e",
   "metadata": {},
   "source": [
    "# OpenAI API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b737ecb-15ba-4757-a85e-59589d7bfdb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === OpenAI API Setup ===\n",
    "client = openai.OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b570341-07ee-439f-b1ae-8b682dd59524",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8eb00-c02d-497f-967d-d1493277eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load review data ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "reviews = df[['review_id', 'review_content_raw_text']].dropna().to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5e48d3b-a0ff-43fc-a155-fdacf7fe8e81",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Load tokenizer ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m encoding = \u001b[43mtiktoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\tiktoken\\model.py:117\u001b[39m, in \u001b[36mencoding_for_model\u001b[39m\u001b[34m(model_name)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencoding_for_model\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m) -> Encoding:\n\u001b[32m    113\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the encoding used by a model.\u001b[39;00m\n\u001b[32m    114\u001b[39m \n\u001b[32m    115\u001b[39m \u001b[33;03m    Raises a KeyError if the model name is not recognised.\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding_name_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\tiktoken\\registry.py:86\u001b[39m, in \u001b[36mget_encoding\u001b[39m\u001b[34m(encoding_name)\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     80\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown encoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoding_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlugins found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_available_plugin_modules()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     82\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtiktoken version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiktoken.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (are you on latest?)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     83\u001b[39m     )\n\u001b[32m     85\u001b[39m constructor = ENCODING_CONSTRUCTORS[encoding_name]\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m enc = Encoding(**\u001b[43mconstructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     87\u001b[39m ENCODINGS[encoding_name] = enc\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m enc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\tiktoken_ext\\openai_public.py:96\u001b[39m, in \u001b[36mo200k_base\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mo200k_base\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     mergeable_ranks = \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m446a9538cb6c348e3516120d7c08b09f57c36495e2acfffe59a5bf8b0cfb1a2d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m     special_tokens = {ENDOFTEXT: \u001b[32m199999\u001b[39m, ENDOFPROMPT: \u001b[32m200018\u001b[39m}\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# This regex could be made more efficient. If I was the one working on this encoding, I would\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# have done a few other things differently too, e.g. I think you can allocate tokens more\u001b[39;00m\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# efficiently across languages.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\tiktoken\\load.py:158\u001b[39m, in \u001b[36mload_tiktoken_bpe\u001b[39m\u001b[34m(tiktoken_bpe_file, expected_hash)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     contents = \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     ret = {}\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m contents.splitlines():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\tiktoken\\load.py:63\u001b[39m, in \u001b[36mread_file_cached\u001b[39m\u001b[34m(blobpath, expected_hash)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m contents = \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m expected_hash \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_hash(contents, expected_hash):\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     66\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHash mismatch for data downloaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblobpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_hash\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis may indicate a corrupted download. Please try again.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\tiktoken\\load.py:22\u001b[39m, in \u001b[36mread_file\u001b[39m\u001b[34m(blobpath)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# avoiding blobfile for public files helps avoid auth issues, like MFA prompts.\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m resp.raise_for_status()\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\requests\\sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\requests\\models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\urllib3\\response.py:1091\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1094\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\urllib3\\response.py:980\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    978\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\urllib3\\response.py:904\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    901\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    906\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VEE0634\\Desktop\\Coding\\garena_game_sourcing_system\\game-sourcing-system\\.venv\\Lib\\site-packages\\urllib3\\response.py:887\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Load tokenizer ===\n",
    "encoding = tiktoken.encoding_for_model(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc603190-6302-4654-9262-a7aa09d93cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load prompt prefix ===\n",
    "with open(\"input/prompt_prefix_1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PROMPT_PREFIX_1 = f.read().strip()\n",
    "\n",
    "with open(\"input/prompt_prefix_2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PROMPT_PREFIX_2 = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669b5f9-a67d-4309-835d-cf4a1626199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Join 2 prefixes together ===\n",
    "PROMPT_PREFIX = PROMPT_PREFIX_1 + \" \" + GAME_URL + \"\\n\\n\" + PROMPT_PREFIX_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bf966-f935-4cd7-8636-55fd548b44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Output file prep ===\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e0227-ea75-49d6-ae12-e0ce8484ac9e",
   "metadata": {},
   "source": [
    "# Estimate cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888971ba-b7c6-4f92-99f0-e3af66ac393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_to_process = unprocessed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c51366-553b-4d3f-85ed-9291783bbc0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Token counting for Input ===\n",
    "base_token_count = count_tokens(PROMPT_PREFIX)\n",
    "total_review_tokens = sum(count_tokens(str(r['review_content_raw_text'])) for r in reviews_to_process)\n",
    "total_tokens = base_token_count + total_review_tokens\n",
    "\n",
    "print(f\"\\n📊 Estimated token usage:\")\n",
    "print(f\"- Prompt prefix: {base_token_count} tokens\")\n",
    "print(f\"- Total reviews: {len(reviews_to_process)}\")\n",
    "print(f\"- Total review content: {total_review_tokens} tokens\")\n",
    "print(f\"- Estimated total prompt tokens: {total_tokens}\")\n",
    "print(f\"- Approximate cost (input only): ${total_tokens / 1000 * 0.005:.4f} (GPT-4o)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cffca5-6947-4afc-9fa4-e69edb33b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Token counting for Output - Processed Reviews ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c656f4-d727-4a1c-b97e-6a386174abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_extract = [\n",
    "    \"review_id\",\n",
    "\n",
    "    \"sensitive_content\",\n",
    "    \"sensitive_content_list\",\n",
    "\n",
    "    \"core_combat_mechanics_description\",\n",
    "    \"combat_satisfaction_rating\",\n",
    "    \"combat_satisfaction_rating_reason\",\n",
    "\n",
    "    \"strategic_tactical_features\",\n",
    "    \"strategic_tactical_depth_rating\",\n",
    "    \"strategic_tactical_depth_rating_reason\",\n",
    "\n",
    "    \"game_progression_experience_rating\",\n",
    "    \"game_progression_experience_rating_reason\",\n",
    "\n",
    "    \"hero_balance_rating\",\n",
    "    \"hero_balance_rating_reason\",\n",
    "\n",
    "    \"hero_team_build_diversity_rating\",\n",
    "    \"hero_team_build_diversity_rating_reason\",\n",
    "\n",
    "    \"secondary_core_loop_description\",\n",
    "    \"secondary_core_loop_being_tycoon_crafting\",\n",
    "    \"secondary_core_loop_simplicity_rating\",\n",
    "    \"secondary_core_loop_simplicity_rating_reason\",\n",
    "    \"abundance_of_resources_earned_from_secondary_core_loop_rating\",\n",
    "    \"abundance_of_resources_earned_from_secondary_core_loop_rating_reason\",\n",
    "\n",
    "    \"new_content_and_meta_evolving_frequency_rating\",\n",
    "    \"new_content_and_meta_evolving_frequency_rating_reason\",\n",
    "\n",
    "    \"gacha_guarantee_mechanism\",\n",
    "    \"gacha_guarantee_mechanism_description\",\n",
    "    \"gacha_pull_price_reasonableness_rating\",\n",
    "    \"gacha_pull_price_reasonableness_rating_reason\",\n",
    "\n",
    "    \"availability_of_major_features_to_non_paying_user_rating\",\n",
    "    \"availability_of_major_features_to_non_paying_user_rating_reason\",\n",
    "    \"pressure_for_spending_rating\",\n",
    "    \"pressure_for_spending_rating_reason\",\n",
    "    \"abundance_and_meaningfulness_of_free_rewards_rating\",\n",
    "    \"abundance_and_meaningfulness_of_free_rewards_rating_reason\",\n",
    "\n",
    "    \"ip_integration\",\n",
    "    \"ip_description\",\n",
    "    \"depth_of_IP_integration_rating\",\n",
    "    \"depth_of_IP_integration_rating_reason\",\n",
    "\n",
    "    \"lightness_of_installation_file_rating\",\n",
    "    \"lightness_of_installation_file_rating_reason\",\n",
    "    \"ingame_downloading_experience_rating\",\n",
    "    \"ingame_downloading_experience_rating_reason\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f598d-4475-48e2-83c3-05a2ebc96aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_reviews_under_new_schema = extract_fields_from_list_of_dicts(\n",
    "    all_processed_reviews,\n",
    "    fields_to_extract\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4b0e7-e6d9-4204-bd6b-d04f7e25143a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_processed_reviews_under_new_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205feed-b673-469c-960b-2af8aff4bd2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_count_all_processed_reviews_under_new_schema = summarize_and_count_tokens(all_processed_reviews_under_new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f6c7a-0bef-4c7e-802a-7cd4c655f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_count_all_processed_reviews_under_new_schema = pd.DataFrame(token_count_all_processed_reviews_under_new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995b250-b30e-454b-a8ae-8770da611224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_count_all_processed_reviews_under_new_schema.to_csv(OUTPUT_DIR + \"/df_token_count_all_processed_reviews_under_new_schema.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934bf8f-1def-4ad4-bdd8-e298a05db063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Token counting for Optimized Output ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee63e1-9269-4dca-89bc-306db93769ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_mapping = {\n",
    "    \"review_id\": \"q0\",\n",
    "\n",
    "    \"sensitive_content\": \"q1\",\n",
    "    \"sensitive_content_list\": \"q2\",\n",
    "\n",
    "    \"core_combat_mechanics_description\": \"q3\",\n",
    "    \"combat_satisfaction_rating\": \"q4\",\n",
    "    \"combat_satisfaction_rating_reason\": \"q5\",\n",
    "\n",
    "    \"strategic_tactical_features\": \"q6\",\n",
    "    \"strategic_tactical_depth_rating\": \"q7\",\n",
    "    \"strategic_tactical_depth_rating_reason\": \"q8\",\n",
    "\n",
    "    \"game_progression_experience_rating\": \"q9\",\n",
    "    \"game_progression_experience_rating_reason\": \"q10\",\n",
    "\n",
    "    \"hero_balance_rating\": \"q11\",\n",
    "    \"hero_balance_rating_reason\": \"q12\",\n",
    "\n",
    "    \"hero_team_build_diversity_rating\": \"q13\",\n",
    "    \"hero_team_build_diversity_rating_reason\": \"q14\",\n",
    "\n",
    "    \"secondary_core_loop_description\": \"q15\",\n",
    "    \"secondary_core_loop_being_tycoon_crafting\": \"q16\",\n",
    "    \"secondary_core_loop_simplicity_rating\": \"q17\",\n",
    "    \"secondary_core_loop_simplicity_rating_reason\": \"q18\",\n",
    "    \"abundance_of_resources_earned_from_secondary_core_loop_rating\": \"q19\",\n",
    "    \"abundance_of_resources_earned_from_secondary_core_loop_rating_reason\": \"q20\",\n",
    "\n",
    "    \"new_content_and_meta_evolving_frequency_rating\": \"q21\",\n",
    "    \"new_content_and_meta_evolving_frequency_rating_reason\": \"q22\",\n",
    "\n",
    "    \"gacha_guarantee_mechanism\": \"q23\",\n",
    "    \"gacha_guarantee_mechanism_description\": \"q24\",\n",
    "    \"gacha_pull_price_reasonableness_rating\": \"q25\",\n",
    "    \"gacha_pull_price_reasonableness_rating_reason\": \"q26\",\n",
    "\n",
    "    \"availability_of_major_features_to_non_paying_user_rating\": \"q27\",\n",
    "    \"availability_of_major_features_to_non_paying_user_rating_reason\": \"q28\",\n",
    "    \"pressure_for_spending_rating\": \"q29\",\n",
    "    \"pressure_for_spending_rating_reason\": \"q30\",\n",
    "    \"abundance_and_meaningfulness_of_free_rewards_rating\": \"q31\",\n",
    "    \"abundance_and_meaningfulness_of_free_rewards_rating_reason\": \"q32\",\n",
    "\n",
    "    \"ip_integration\": \"q33\",\n",
    "    \"ip_description\": \"q34\",\n",
    "    \"depth_of_IP_integration_rating\": \"q35\",\n",
    "    \"depth_of_IP_integration_rating_reason\": \"q36\",\n",
    "\n",
    "    \"lightness_of_installation_file_rating\": \"q37\",\n",
    "    \"lightness_of_installation_file_rating_reason\": \"q38\",\n",
    "    \"ingame_downloading_experience_rating\": \"q39\",\n",
    "    \"ingame_downloading_experience_rating_reason\": \"q40\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1c7dd-3bba-4f06-b996-a3eb87322196",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_reviews_under_new_schema_optimized = remap_keys(all_processed_reviews_under_new_schema, key_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0846af-b318-42d7-8fc1-dcaa4b5c931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count_all_processed_reviews_under_new_schema_optimized = summarize_and_count_tokens(all_processed_reviews_under_new_schema_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd01f9-ce17-4dab-9efe-9fc0a6aaa961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_count_all_processed_reviews_under_new_schema_optimized = pd.DataFrame(token_count_all_processed_reviews_under_new_schema_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80538fb5-48ac-4e16-9f5d-b893f2b94073",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_token_count_all_processed_reviews_under_new_schema_optimized.to_csv(OUTPUT_DIR + \"/df_token_count_all_processed_reviews_under_new_schema_optimized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14eef5a-5d60-4f42-9b80-b5262a3e3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Token counting for Input - Unprocessed Reviews ===\n",
    "\n",
    "token_count_input_unprocessed_reviews = [\n",
    "    {\n",
    "        \"review_id\": review[\"review_id\"],\n",
    "        \"token_count\": count_tokens(review[\"review_content_raw_text\"])\n",
    "    }\n",
    "    for review in unprocessed_reviews\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cf747d-30e6-469f-9953-76cc4031a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_count_input_unprocessed_reviews = pd.DataFrame(token_count_input_unprocessed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391c3bb-ab3d-4a4f-9172-3d970306ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_count_input_unprocessed_reviews.to_csv(\"output/df_token_count_input_unprocessed_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3454132-2120-4faf-b2d3-751823ac4cae",
   "metadata": {},
   "source": [
    "# Batch Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a3784-a2e2-4dda-bc2c-cf1d609c6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Batch reviews by token limit ===\n",
    "batches = batch_reviews(unprocessed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac0ac0-48d6-4220-ae6b-111575667234",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d74ab7-b624-4012-ad1a-714df9690179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c86e90-52e5-4f47-baff-1878fae94710",
   "metadata": {},
   "source": [
    "# Prompting: Batch by Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0b8c9-78b4-4642-847c-94d1b1d090e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec7ef5-d0f3-4cc7-b514-b1e2828e1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c508d-1050-4ab2-842c-a58376d4b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n🚀 Sending batch {i+1}/{len(batches)}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81dffd-63ec-432f-a573-91a3865d73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "batch_prompt = PROMPT_PREFIX + \"\\n\\nINPUT REVIEWS TO ANALYZE (NOTE THAT THERE COULD BE MULTIPLE REVIEWS)\\n\" + json.dumps(batch, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c2b7e-93f1-4270-84da-11c8033fce02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(batch_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06438801-73ef-4829-9184-e83c1e28b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call API\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": batch_prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ OpenAI API error in batch {i+1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677b2f8-5894-4536-b434-f8a9e1586615",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16817bc1-5c06-4956-beca-aa125677de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw content for backup/debug\n",
    "raw_output_file = os.path.join(\"output/iter_3\", f\"batch_{i+1:03d}_raw.txt\")\n",
    "with open(raw_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49d3d8-ab82-4ef8-a02a-beeb8a230c12",
   "metadata": {},
   "source": [
    "# Prompting: Many Batch in a Batch Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3cdf7-9b33-4be9-89d3-b7aad6b3b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_for_returning_text = OUTPUT_DIR_ITER_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7906aa-ceea-4b42-820a-d9c8f637e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ff689-8370-4d2e-8639-d15adc6b44c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(start_i, len(batches)):\n",
    "\n",
    "    batch = batches[i]\n",
    "    \n",
    "    print(f\"\\n🚀 Sending batch {i+1}/{len(batches)}...\")\n",
    "    \n",
    "    # Build prompt\n",
    "    batch_prompt = PROMPT_PREFIX + \"\\n\\nINPUT REVIEWS TO ANALYZE (NOTE THAT THERE COULD BE MULTIPLE REVIEWS)\\n\" + json.dumps(batch, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Call API\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": batch_prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ OpenAI API error in batch {i+1}: {e}\")\n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Save raw content for backup/debug\n",
    "    raw_output_file = os.path.join(output_dir_for_returning_text, f\"batch_{i+1:03d}_raw.txt\")\n",
    "    with open(raw_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc32e7-26ca-4584-bc48-3bca6b9f80f2",
   "metadata": {},
   "source": [
    "# Read Prompting Text File Output into Python list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e40af-7598-4e75-a830-7a2a832cf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_batches_from_folder(folder_path, file_prefix=\"batch_\", file_suffix=\"_raw\", extension=\".txt\"):\n",
    "    \"\"\"\n",
    "    Loads and parses all .txt batch files from a folder into a single list,\n",
    "    handling markdown ```json fences and trailing ``` if present.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing the batch files.\n",
    "        file_prefix (str): Common prefix for batch files (default: \"batch_\").\n",
    "        file_suffix (str): Suffix after batch number (default: \"_raw\").\n",
    "        extension (str): File extension (default: \".txt\").\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Combined list of all parsed review entries from all batches.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.startswith(file_prefix) and filename.endswith(file_suffix + extension):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            print(f\"📂 Reading: {filename}\")\n",
    "            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw = f.read().strip()\n",
    "\n",
    "                # ✅ Clean markdown-style wrapping if present\n",
    "                if raw.startswith(\"```json\"):\n",
    "                    raw = raw[len(\"```json\"):].strip()\n",
    "                elif raw.startswith(\"```\"):\n",
    "                    raw = raw[len(\"```\"):].strip()\n",
    "\n",
    "                if raw.endswith(\"```\"):\n",
    "                    raw = raw[:-3].strip()\n",
    "                    \n",
    "                # Fix TRUE/FALSE casing\n",
    "                raw = raw.replace(\"TRUE\", \"true\").replace(\"FALSE\", \"false\")\n",
    "\n",
    "                # ✅ Parse JSON safely\n",
    "                try:\n",
    "                    data = json.loads(raw)\n",
    "                    if isinstance(data, list):\n",
    "                        all_data.extend(data)\n",
    "                    else:\n",
    "                        print(f\"⚠️ Skipped (not a list): {filename}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"❌ Failed to parse JSON from {filename}: {e}\")\n",
    "    \n",
    "    print(f\"\\n✅ Total reviews loaded: {len(all_data)}\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46d9b4-d4d1-4112-8c64-98b880e2d872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_analysis_result = load_all_batches_from_folder(OUTPUT_DIR_ITER_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825c442-a52e-46be-8c6a-608dde86ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_analysis_results_all = pd.DataFrame(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed560c-53ea-4344-903d-0c43e18b1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_review_analysis_results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef3e056-b466-4f7d-9e0d-10eed2045184",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = [f\"q{i}\" for i in range(0, 42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ab09d-533f-4a41-ac1c-81c974f52563",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_analysis_results_all = df_review_analysis_results_all[column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f420c94-a0a3-446a-b76f-13b17eb700dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_review_analysis_results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ff4dc-e3e8-4c6f-a816-2776c44084d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_analysis_results_all.to_csv(OUTPUT_DIR + \"/df_review_analysis_results_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401d14e-68c3-4a1e-b18b-2ed396c7484d",
   "metadata": {},
   "source": [
    "# 1st Prompting (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d76f9-2afd-4625-81af-b1616e4c4aa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Send batches and append to single output file ===\n",
    "for i, batch in enumerate(batches):\n",
    "    print(f\"\\n🚀 Sending batch {i+1}/{len(batches)}...\")\n",
    "\n",
    "    # Build prompt\n",
    "    batch_prompt = PROMPT_PREFIX + \"\\n\\nINPUT REVIEWS TO ANALYZE (NOTE THAT THERE COULD BE MULTIPLE REVIEWS)\\n\" + json.dumps(batch, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Call API\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": batch_prompt}],\n",
    "            temperature=0.2\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ OpenAI API error in batch {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Debug output (optional)\n",
    "    print(f\"\\n🧾 Cleaned GPT output for batch {i+1}:\\n{content[:500]}...\\n\")\n",
    "    \n",
    "    # Save raw content for backup/debug\n",
    "    raw_output_file = os.path.join(OUTPUT_DIR, f\"batch_{i+1:03d}_raw.txt\")\n",
    "    with open(raw_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    # ✅ Strip leading ```json or ``` if present\n",
    "    if content.startswith(\"```json\"):\n",
    "        content = re.sub(r\"^```json\\s*\", \"\", content)\n",
    "        content = re.sub(r\"\\s*```$\", \"\", content)\n",
    "    elif content.startswith(\"```\"):\n",
    "        content = re.sub(r\"^```\\s*\", \"\", content)\n",
    "        content = re.sub(r\"\\s*```$\", \"\", content)\n",
    "\n",
    "    # Parse new result\n",
    "    try:\n",
    "        new_results = json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ Failed to parse JSON from batch {i+1}: {e}\")\n",
    "        print(f\"⚠️ Saved raw response to: {raw_output_file}\")\n",
    "        continue\n",
    "\n",
    "    # Load existing file if exists\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                all_results = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                all_results = []\n",
    "    else:\n",
    "        all_results = []\n",
    "\n",
    "    # Validate each item before appending\n",
    "    valid_results = []\n",
    "    invalid_results = []\n",
    "    \n",
    "    for idx, item in enumerate(new_results):\n",
    "        try:\n",
    "            validate(instance=item, schema=review_schema)\n",
    "            valid_results.append(item)\n",
    "        except ValidationError as ve:\n",
    "            print(f\"❌ Validation failed for review index {idx} in batch {i+1}: {ve.message}\")\n",
    "            invalid_results.append(item)\n",
    "\n",
    "    # Append and write only valid results\n",
    "    if valid_results:\n",
    "        all_results.extend(valid_results)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✅ Appended {len(valid_results)} valid results from batch {i+1}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No valid results to append from batch {i+1}\")\n",
    "        \n",
    "print(\"\\n🎉 All reviews processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30387826-6a68-4069-913a-ba403cdeb136",
   "metadata": {},
   "source": [
    "# Process analyzed reviews & Filter out unanalyzed ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb286d-97d6-4d5f-8866-43ad71f69ef5",
   "metadata": {},
   "source": [
    "New one: the 3061 reviews analyzed using the version of April 14 (results in Iter 3 folder) are considered processed reviews (leaving out the other 500 reviews in Iter 1 folder as the prompt has changed a lot since then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0470a3c-b399-430c-a4db-596da0e17d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RESULT COLLECTOR ===\n",
    "all_processed_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749091c-bf57-4262-9cc3-38bf7e714042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === CLEAN + PARSE EACH FILE ===\n",
    "for file in sorted(os.listdir(OUTPUT_DIR_ITER_9)):\n",
    "    if file.startswith(RAW_PREFIX) and file.endswith(RAW_SUFFIX):\n",
    "        full_path = os.path.join(OUTPUT_DIR_ITER_9, file)\n",
    "        print(f\"🔍 Processing: {file}\")\n",
    "\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw = f.read().strip()\n",
    "\n",
    "        # Strip markdown wrapper\n",
    "        if raw.startswith(\"```json\"):\n",
    "            raw = re.sub(r\"^```json\\s*\", \"\", raw)\n",
    "            raw = re.sub(r\"\\s*```$\", \"\", raw)\n",
    "        elif raw.startswith(\"```\"):\n",
    "            raw = re.sub(r\"^```\\s*\", \"\", raw)\n",
    "            raw = re.sub(r\"\\s*```$\", \"\", raw)\n",
    "\n",
    "        # Fix TRUE/FALSE casing\n",
    "        raw = raw.replace(\"TRUE\", \"true\").replace(\"FALSE\", \"false\")\n",
    "\n",
    "        # Try to parse as partial JSON list\n",
    "        try:\n",
    "            # Attempt full load first\n",
    "            parsed = json.loads(raw)\n",
    "            if isinstance(parsed, list):\n",
    "                all_processed_reviews.extend(parsed)\n",
    "            else:\n",
    "                print(f\"⚠️ {file} is not a list. Skipped.\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            # Try partial rescue if JSON is truncated\n",
    "            print(f\"❌ Failed to load full JSON from {file} — trying to salvage valid objects...\")\n",
    "            try:\n",
    "                # Match individual JSON objects\n",
    "                partial_objects = re.findall(r'{.*?}(?=,|\\s*\\])', raw, re.DOTALL)\n",
    "                for obj in partial_objects:\n",
    "                    try:\n",
    "                        review = json.loads(obj)\n",
    "                        all_processed_reviews.append(review)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                print(f\"✅ Salvaged {len(partial_objects)} items from {file}\")\n",
    "            except Exception as ex:\n",
    "                print(f\"❌ Failed to salvage anything from {file}: {ex}\")\n",
    "\n",
    "print(f\"\\n✅ Total valid reviews collected: {len(all_processed_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41be603-c866-4dd8-ac91-029d29ea44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON file\n",
    "with open(\"output/final_results/result_set_007.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_processed_reviews, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27997759-3dbf-4ab6-ad95-bce8a6997859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine existing result files\n",
    "\n",
    "# Load first JSON file\n",
    "with open(\"output/final_results/result_set_001.json\", \"r\", encoding=\"utf-8\") as f1:\n",
    "    data1 = json.load(f1)\n",
    "\n",
    "# Load first JSON file\n",
    "with open(\"output/final_results/result_set_002.json\", \"r\", encoding=\"utf-8\") as f2:\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Load first JSON file\n",
    "with open(\"output/final_results/result_set_003.json\", \"r\", encoding=\"utf-8\") as f3:\n",
    "    data3 = json.load(f3)\n",
    "\n",
    "# Load first JSON file\n",
    "with open(\"output/final_results/result_set_004.json\", \"r\", encoding=\"utf-8\") as f4:\n",
    "    data4 = json.load(f4)\n",
    "\n",
    "# Load first JSON file\n",
    "with open(\"output/final_results/result_set_005.json\", \"r\", encoding=\"utf-8\") as f5:\n",
    "    data5 = json.load(f5)\n",
    "\n",
    "# Load first JSON file\n",
    "with open(\"output/final_results/result_set_006.json\", \"r\", encoding=\"utf-8\") as f6:\n",
    "    data6 = json.load(f6)\n",
    "\n",
    "# Load first JSON file\n",
    "with open(\"output/final_results/result_set_007.json\", \"r\", encoding=\"utf-8\") as f7:\n",
    "    data7 = json.load(f7)\n",
    "\n",
    "# Combine the two lists\n",
    "combined_data = data1 + data2 + data3 + data4 + data5 + data6 + data7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f75062-0039-4275-bab5-db07f1df6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract processed review_ids into a set for fast lookup\n",
    "processed_ids = {r[\"q0\"] for r in combined_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388bcf6-db77-40dc-99ca-54c4a98d6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out unprocessed reviews\n",
    "unprocessed_reviews = [r for r in reviews if r[\"review_id\"] not in processed_ids]\n",
    "\n",
    "print(f\"🔍 Total reviews in original set: {len(reviews)}\")\n",
    "print(f\"✅ Processed reviews: {len(combined_data)}\")\n",
    "print(f\"❗ Unprocessed reviews: {len(unprocessed_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269bd0c-9b49-4acf-9d5a-17ccf34d2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Convert List of Unprocessed Reviews to DataFrame ===\n",
    "df = pd.DataFrame(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e86418-f95f-4199-83ec-22ad764d3bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save List of Unprocessed Reviews to CSV ===\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")  # Use utf-8-sig for Excel compatibility\n",
    "\n",
    "print(f\"📁 Saved all processed reviews as CSV to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae48a2-6929-4d3b-b025-921f49929623",
   "metadata": {},
   "source": [
    "# Generate Report from Analyzed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fc439-db35-41bb-b53b-c076a5075a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the analyzed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129444a-4ddc-42be-8622-006c70a5eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_analysis_results_for_synthesis = pd.read_csv(\"output/df_review_analysis_results_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f172579-41b2-46e0-b242-b003dc2e4474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_review_analysis_results_for_synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5813feb3-21c6-46ad-966e-36cd99f9b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8390cc-31de-4574-a07b-a7841289b323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2263f-1f3a-48c0-98e9-363dd1628d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_review_analysis_results_for_synthesis = pd.merge(\n",
    "    df_review_analysis_results_for_synthesis,\n",
    "    df_reviews,\n",
    "    left_on=\"q0\",\n",
    "    right_on=\"review_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250af89e-4954-405d-9079-3e6fb08e996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of question content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46794a7b-7987-437e-b26c-5a6a4e42286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_content = {\n",
    "    \"q1\": \"Does the game contain any sensitive content (Sexual Content/Nudity, Violence/Gore, Drugs/Alcohol/Tobacco, Religious/Political)? (If not mentioned, answer “N/A”)\",\n",
    "    \"q2\": \"List the sensitive content mentioned, if any.\",\n",
    "    \"q3\": \"Summarize the core combat mechanics (or “N/A” if not mentioned).\",\n",
    "    \"q4\": \"Rate combat satisfaction: -2 (very negative) to 2 (very positive) or “N/A”\",\n",
    "    \"q5\": \"Reason for previous question’s rating\",\n",
    "    \"q6\": \"List the strategic/tactical features mentioned, if any.\",\n",
    "    \"q7\": \"Rate strategic/tactical depth: -2 to 2 or “N/A”\",\n",
    "    \"q8\": \"Reason for previous question’s rating\",\n",
    "    \"q9\": \"Rate game progression: -2 to 2 or “N/A”\",\n",
    "    \"q10\": \"Reason for previous question’s rating\",\n",
    "    \"q11\": \"Rate hero balance: -2 to 2 or “N/A”\",\n",
    "    \"q12\": \"Reason for previous question’s rating\",\n",
    "    \"q13\": \"Rate hero/team build diversity: -2 to 2 or “N/A”\",\n",
    "    \"q14\": \"Reason for previous question’s rating\",\n",
    "    \"q15\": \"Describe the secondary core loop (or “N/A”)\",\n",
    "    \"q16\": \"Does the secondary loop match tycoon/crafting (e.g. Township, Hay Day)? (True/False/“N/A”)\",\n",
    "    \"q17\": \"Rate simplicity of the secondary loop: -2 to 2 or “N/A”\",\n",
    "    \"q18\": \"Reason for previous question’s rating\",\n",
    "    \"q19\": \"Rate resources earned from the secondary loop: -2 to 2 or “N/A”\",\n",
    "    \"q20\": \"Reason for previous question’s rating\",\n",
    "    \"q21\": \"Rate frequency of content/meta updates: -2 to 2 or “N/A”\",\n",
    "    \"q22\": \"Reason for previous question’s rating\",\n",
    "    \"q23\": \"Does the game have a gacha guarantee system? (True/False/“N/A”)\",\n",
    "    \"q24\": \"Briefly describe the gacha guarantee system if answer to previous question is True, else answer “N/A”\",\n",
    "    \"q25\": \"Rate reasonableness of gacha pull price: -2 to 2 or “N/A”\",\n",
    "    \"q26\": \"Reason for previous question’s rating\",\n",
    "    \"q27\": \"Rate major feature access for non-paying users: -2 to 2 or “N/A”\",\n",
    "    \"q28\": \"Reason for previous question’s rating\",\n",
    "    \"q29\": \"Rate spending pressure: -2 (heavy/annoying) to 2 (elegant/subtle), or “N/A”\",\n",
    "    \"q30\": \"Reason for previous question’s rating\",\n",
    "    \"q31\": \"Rate quality/quantity of free rewards: -2 to 2 or “N/A”\",\n",
    "    \"q32\": \"Reason for previous question’s rating\",\n",
    "    \"q33\": \"Does the game integrate IP? (True/False if says none/“N/A” if not mentioned). Briefly describe.\",\n",
    "    \"q34\": \"Briefly describe the IP if answer to previous question is True, else answer “N/A”\",\n",
    "    \"q35\": \"Rate IP integration depth: -2 to 2 or “N/A”\",\n",
    "    \"q36\": \"Reason for previous question’s rating\",\n",
    "    \"q37\": \"Rate lightness of installation file: -2 to 2 or “N/A”\",\n",
    "    \"q38\": \"Reason for previous question’s rating\",\n",
    "    \"q39\": \"Rate in-game download experience: -2 to 2 or “N/A”\",\n",
    "    \"q40\": \"Reason for previous question’s rating\",\n",
    "    \"q41\": \"Briefly list other issues (not captured by the questions above)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2a83f-4e48-4813-b851-86853a3b06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e7a774-331e-4fd4-9b40-615872c00334",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_types = {\n",
    "    \"q1\": \"true_false\",\n",
    "    \"q2\": \"long_form\",\n",
    "    \"q3\": \"long_form\",\n",
    "    \"q4\": \"integer_rating\",\n",
    "    \"q5\": \"long_form\",\n",
    "    \"q6\": \"long_form\",\n",
    "    \"q7\": \"integer_rating\",\n",
    "    \"q8\": \"long_form\",\n",
    "    \"q9\": \"integer_rating\",\n",
    "    \"q10\": \"long_form\",\n",
    "    \"q11\": \"integer_rating\",\n",
    "    \"q12\": \"long_form\",\n",
    "    \"q13\": \"integer_rating\",\n",
    "    \"q14\": \"long_form\",\n",
    "    \"q15\": \"long_form\",\n",
    "    \"q16\": \"true_false\",\n",
    "    \"q17\": \"integer_rating\",\n",
    "    \"q18\": \"long_form\",\n",
    "    \"q19\": \"integer_rating\",\n",
    "    \"q20\": \"long_form\",\n",
    "    \"q21\": \"integer_rating\",\n",
    "    \"q22\": \"long_form\",\n",
    "    \"q23\": \"true_false\",\n",
    "    \"q24\": \"long_form\",\n",
    "    \"q25\": \"integer_rating\",\n",
    "    \"q26\": \"long_form\",\n",
    "    \"q27\": \"integer_rating\",\n",
    "    \"q28\": \"long_form\",\n",
    "    \"q29\": \"integer_rating\",\n",
    "    \"q30\": \"long_form\",\n",
    "    \"q31\": \"integer_rating\",\n",
    "    \"q32\": \"long_form\",\n",
    "    \"q33\": \"true_false\",\n",
    "    \"q34\": \"long_form\",\n",
    "    \"q35\": \"integer_rating\",\n",
    "    \"q36\": \"long_form\",\n",
    "    \"q37\": \"integer_rating\",\n",
    "    \"q38\": \"long_form\",\n",
    "    \"q39\": \"integer_rating\",\n",
    "    \"q40\": \"long_form\",\n",
    "    \"q41\": \"long_form\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772df358-c631-445c-b50b-e6784f843b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all synthesis prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39dec5-d529-4ec2-beb3-48d31b110dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = \"\"\"Rules:\n",
    "- Output bulleted points only (no paragraphs, no JSON).\n",
    "- Group points into short, factual insights (≤20 words each).\n",
    "- Each bullet must end with review IDs in square brackets, e.g. [37988997, 35631039].\n",
    "- Include ≤1 short quote (≤12 words) if useful for clarity.\n",
    "- Output must strictly be in English\n",
    "- If INPUT is empty, output: • No reviews matched.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf4dc8-5b2d-4de8-ae72-d67ae724cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_prompts_core_parts = {\n",
    "    \"q2\":\"\"\"You are an analyst. ONLY use the reviews in INPUT. Summarize what players say about sensitive content.\n",
    "Sensitive categories: Sexual Content/Nudity, Violence/Gore, Drugs/Alcohol/Tobacco, Religious/Political.\"\"\",\n",
    "    \"q3\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize what players say about the game's core combat mechanics.\n",
    "Definition: Core combat mechanics = primary battle systems, controls, pacing, balance, skill systems, roles/classes, resource usage in combat, and related player strategy.\"\"\",\n",
    "    \"q5\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on combat satisfaction rating.\"\"\",\n",
    "    \"q6\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "List the strategic/tactical features mentioned by players.\"\"\",\n",
    "    \"q8\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on strategic/tactical depth.\"\"\",\n",
    "    \"q10\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on progression system.\"\"\",\n",
    "    \"q12\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on hero balance.\"\"\",\n",
    "    \"q14\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on hero/team build diversity.\"\"\",\n",
    "    \"q15\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Describe the secondary core loop of the game as mentioned by players.\"\"\",\n",
    "    \"q18\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on the simplicity rating of the secondary loop.\"\"\",\n",
    "    \"q20\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on the resources earned from the secondary loop.\"\"\",\n",
    "    \"q22\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on the frequency of content/meta updates.\"\"\",\n",
    "    \"q24\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Briefly describe the gacha guarantee system as mentioned by players.\"\"\",\n",
    "    \"q26\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on the gacha pull price reasonableness.\"\"\",\n",
    "    \"q28\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on major feature access for non-paying users.\"\"\",\n",
    "    \"q30\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on spending pressure.\"\"\",\n",
    "    \"q32\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on free rewards quality/quantity.\"\"\",\n",
    "    \"q34\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Briefly describe the IP integrated into the game as mentioned by players.\"\"\",\n",
    "    \"q36\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on IP integration depth.\"\"\",\n",
    "    \"q38\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on lightness of installation file.\"\"\",\n",
    "    \"q40\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "Summarize the players' opinion on in-game download experience.\"\"\",\n",
    "    \"q41\":\"\"\"You are an analyst. ONLY use the reviews provided in INPUT.\n",
    "List issues mentioned by the players other than the following issues: sensitive content, core combat mechanics, strategic/tactical features, game progression, hero balance, hero/team build diversity, secondary game loop, frequency of content/meta updates, gacha guarantee system, gacha pull price, feature access for non-paying users, quality/quantity of free rewards, IP integration, lightness of installation file, in-game download experience\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bea3e2-85cc-4d8c-b729-371aee2fa183",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_prompts = dict()\n",
    "\n",
    "for q, p in synthesis_prompts_core_parts.items():\n",
    "    current_full_prompt = p + \"\\n \\n\" + rules\n",
    "    synthesis_prompts[q] = current_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a600e-84f5-4e3b-8b7f-8e32c76fe9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for synthesizing the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1b642-e3cb-4476-bc53-493c89ee4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_answers(df_all_answers, review_id_column_name, review_content_column_name, question_types, synthesis_prompts):\n",
    "\n",
    "    synthesized_answers = dict()\n",
    "    \n",
    "    for q in question_types.keys():\n",
    "        print(\"Synthesizing answers for question: {} - question type: {}\".format(q, question_types[q]))\n",
    "        \n",
    "        if question_types[q] == \"true_false\":\n",
    "            current_synthesized_answer = count_column_values(df_all_answers, q)\n",
    "            print(\"Synthesizing Successful!\")\n",
    "            \n",
    "        elif question_types[q] == \"integer_rating\":\n",
    "            current_synthesized_answer = {\n",
    "                \"overall_score\": calc_weighted_average(df_all_answers, q),\n",
    "                \"detailed_scoring\": count_column_values(df_all_answers, q)\n",
    "            }\n",
    "            print(\"Synthesizing Successful!\")\n",
    "            \n",
    "        else:\n",
    "            # synthesizing long-form answers\n",
    "            \n",
    "            # prepare the table of review ids and review content for the reviews that touch on the topic of the question\n",
    "            df_rows_with_nonna_values_for_current_question = df_all_answers[\n",
    "                df_all_answers[q].notna()\n",
    "            ][[review_id_column_name, review_content_column_name]]\n",
    "\n",
    "            relevant_reviews = df_rows_with_nonna_values_for_current_question.to_dict(orient=\"records\")\n",
    "\n",
    "            # synthesize the answers with AI but using the review content itself rather than the current answers => go directly again to the review content for better reliability\n",
    "            current_synthesized_answer = synthesize_long_form_answers_with_ai(relevant_reviews, synthesis_prompts[q])\n",
    "            \n",
    "            # notification\n",
    "            print(\"Synthesizing Successful!\")\n",
    "\n",
    "        synthesized_answers[q] = current_synthesized_answer\n",
    "\n",
    "    return synthesized_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6ddb0-f03e-4883-8fa4-dd3ab19a9ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7972ab-e939-4fea-ab9e-9beb57d9486d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synthesized_answers = synthesize_answers(\n",
    "    df_review_analysis_results_for_synthesis,\n",
    "    \"review_id\",\n",
    "    \"review_content_raw_text\",\n",
    "    question_types,\n",
    "    synthesis_prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c04d9-b94b-48f7-b451-6fc3493350d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the question content to the synthesis result as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872a849-5d79-4c92-9be5-9c3f45320060",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesized_answers_with_question_content = []\n",
    "\n",
    "for question_code, answer_content in synthesized_answers.items():\n",
    "    current_data_piece = {\n",
    "        'question_code':question_code,\n",
    "        'question_content': question_content[question_code],\n",
    "        'synthesis_answer': answer_content\n",
    "    }\n",
    "    synthesized_answers_with_question_content.append(current_data_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b8ecc-6e4c-4f13-acef-9f392a840c20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synthesized_answers_with_question_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f306ad0e-5d97-42c9-940c-ef9257e287db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/synthesis_results/synthesized_answers_with_question_content.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(synthesized_answers_with_question_content, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50804a0d-d416-401c-95b9-2a1cd75b0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthesized_answers = pd.DataFrame(synthesized_answers_with_question_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fe53c-6f2b-47bf-977b-0adda1635459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_synthesized_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b50c8-edcf-42e6-87c1-b783db61fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthesized_answers.to_csv(\"output/synthesis_results/df_synthesized_answers.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588a6d0-a6d8-40ac-b383-6ebba8d85567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try synthesizing question 1 and question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312bd3a-cffa-4542-ad1e-3c8f2824195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_q1_answers = df_review_analysis_results_for_synthesis[\"q1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efce089-2c73-496c-98c1-75612d11348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_column_values(df_review_analysis_results_for_synthesis, \"q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600703e7-9ded-421c-a1f1-f7c3efeaa1dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_q1_true_reviews = df_review_analysis_results_for_synthesis[\n",
    "   df_review_analysis_results_for_synthesis[\"q1\"]==True\n",
    "][[\"q0\",\"review_content_raw_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394da729-a0fc-491a-9077-5bba17f97108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_q1_true_reviews = df_q1_true_reviews.rename(\n",
    "    columns={\n",
    "        \"q0\":\"review_id\",\n",
    "        \"review_content_raw_text\":\"review_content\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ab605-8586-485c-9b53-35f1249cedb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q1_true_reviews = df_q1_true_reviews.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce8a16b-e5e6-42e0-b10b-2371bc8c2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_summary_result = synthesize_long_form_answers_with_ai(q1_true_reviews, synthesis_prompts[\"q1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb55af-3c86-4504-9bb3-a7052194d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try synthesizing question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af389d88-38a0-456b-9c0c-ae0c8cc618ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_column_values(df_review_analysis_results_for_synthesis, \"q3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1c8f6-8ca1-4928-a23f-19b9663bf023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q3_true_reviews = df_review_analysis_results_for_synthesis[\n",
    "   df_review_analysis_results_for_synthesis[\"q3\"].notna()\n",
    "][[\"q0\",\"review_content_raw_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618bce9-bc7f-47bf-9491-31d0ac8067b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q3_true_reviews = df_q3_true_reviews.rename(\n",
    "    columns={\n",
    "        \"q0\":\"review_id\",\n",
    "        \"review_content_raw_text\":\"review_content\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217ad26-4a2d-43df-ab1f-f89c95a37cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_true_reviews = df_q3_true_reviews.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c9c1f-ae52-46ed-b873-8f6331869afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_summary_result = synthesize_long_form_answers_with_ai(q3_true_reviews, synthesis_prompts[\"q3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa56b7-2e29-4982-9e8d-8f57dbf5310c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(q3_summary_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748dd8f8-bb6b-4b2f-86d5-ef8a0195be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try synthesizing question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d67bc5-6fb7-4e69-a524-cd93c233647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q5_relevant_reviews = df_review_analysis_results_for_synthesis[\n",
    "   df_review_analysis_results_for_synthesis[\"q5\"].notna()\n",
    "][[\"q0\",\"review_content_raw_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720e427-6646-4a65-a995-0da5fd7f1a70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q5_relevant_reviews = df_q5_relevant_reviews.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba5d69-f281-413d-8930-9d772b63a6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q5_summary_result = synthesize_long_form_answers_with_ai(q5_relevant_reviews, synthesis_prompts[\"q5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acbfdde-aa8e-4403-aff0-daea52e32afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(q5_summary_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ed8da-8175-4fb5-b05c-699b432a4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try synthesizing question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc099b0-fec9-422f-9e01-0623b5f4e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_column_values(df_review_analysis_results_for_synthesis, \"q4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf3383-855f-4104-b1cf-934ec8866670",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_weighted_average(df_review_analysis_results_for_synthesis, \"q11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e429dab-3c42-445e-8e28-92c0cd76e6b2",
   "metadata": {},
   "source": [
    "# Generate Summary Report from Detailed Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8507de-a3f1-4e47-81db-a08540dbe1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the detailed report (json format)\n",
    "file_path = Path(\"output/synthesis_results/synthesized_answers_with_question_content.json\")\n",
    "\n",
    "# Load as Python dict/list\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    synthesized_answers_with_question_content = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09082832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the question-answer pairs into a big context string\n",
    "\n",
    "context_string = \"\"\n",
    "\n",
    "for qna in synthesized_answers_with_question_content:\n",
    "    context_string = context_string + \"\\n\\n\" + qna[\"question_code\"] + \": \" + qna[\"question_content\"] + \"\\n\" + str(qna[\"synthesis_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b71426",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DETAILED_TO_GENERAL_SUMMARY = \"gpt-5\"\n",
    "SYSTEM_INSTRUCTIONS_DETAILED_TO_GENERAL_SUMMARY = (\n",
    "    \"You are a senior game analyst. Write concise, English-only commentary.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc8b2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_from_text_detailed_to_general_summary(game_link: str, qa_block_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Builds the exact prompt string you provided, inserting the game link and the\n",
    "    raw 'THE ANALYSIS ANSWERS:' text block (already formatted with q1..qN).\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"Below are the answers to questions used to analyze user reviews on \"\n",
    "        f\"taptap.cn for this game: {game_link} \"\n",
    "        \"Please help to summarize these analysis answers into a brief commentary of the game. \"\n",
    "        \"The commentary should entirely be in English and its structure should be:\\n\"\n",
    "        \"- Overall conclusion\\n\"\n",
    "        \"- Breakdown:\\n\"\n",
    "        \"   + Sensitive Content (q1 - q2)\\n\"\n",
    "        \"   + Gameplay (q3 - q20)\\n\"\n",
    "        \"   + LiveOps & Retention Systems (q21 - q22)\\n\"\n",
    "        \"   + Monetization & Game Economy (q23 - q30)\\n\"\n",
    "        \"   + Anti-P2W Mechanism (q31 - q32)\\n\"\n",
    "        \"   + IP Integration (q33 - q36)\\n\"\n",
    "        \"   + System Requirement (q37 - q40)\\n\"\n",
    "        \"   + Other Issues (q41)\\n\\n\"\n",
    "        \"THE ANALYSIS ANSWERS:\\n\"\n",
    "        f\"{qa_block_text}\".strip()\n",
    "    )\n",
    "\n",
    "def get_commentary_from_text_detailed_to_general_summary(game_link: str, qa_block_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends one request to the OpenAI Responses API and returns the model's final concatenated text.\n",
    "    \"\"\"\n",
    "\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "    prompt_text = build_prompt_from_text_detailed_to_general_summary(game_link, qa_block_text)\n",
    "\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL_DETAILED_TO_GENERAL_SUMMARY,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_INSTRUCTIONS_DETAILED_TO_GENERAL_SUMMARY }],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": prompt_text}],\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return resp.output_text  # convenient final text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec539f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_summary = get_commentary_from_text_detailed_to_general_summary(GAME_URL, context_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2095f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall conclusion\n",
      "A portrait, turn-based pet RPG with approachable one‑hand play and some squad-building depth, but dragged down by slow pacing, shaky class/pet balance, grindy progression, and heavy monetization pressure. LiveOps frequently nerf rewards and introduce instability, while the client is large and download/patching is disruptive. Cultural-Tang IP flavor lands for some, but integration and certain aesthetic choices draw criticism.\n",
      "\n",
      "Breakdown:\n",
      "- Sensitive Content (q1 - q2)\n",
      "  • Minor sexualized female designs (cleavage/physics).  \n",
      "  • Cultural/religious sensitivity complaints about Japanese elements within a Tang-era setting.  \n",
      "  • No notable mentions of gore or substances.\n",
      "\n",
      "- Gameplay (q3 - q20)\n",
      "  • Core combat: classic turn-based with portrait ergonomics, manual/auto options, and pet/spirit evolution as a standout.  \n",
      "  • Satisfaction: mixed/neutral; praised for strategy but widely called slow and repetitive, with requests for speed-up/skip and smarter auto.  \n",
      "  • Strategic depth: modestly positive; build/targeting and pet/skill synergies matter, but PvE repetition and limited innovation cap depth.  \n",
      "  • Progression: tilts negative; grindy dailies, increasing resource gates, difficulty spikes, and perceived pay advantage.  \n",
      "  • Balance: below par; physical vs. magical disparity, outlier classes/pets (e.g., Gu Ling) dominate; frequent reactive buffs/nerfs.  \n",
      "  • Team/build diversity: slightly positive on paper (roles/classes/pets), but meta dominance constrains viable comps.  \n",
      "  • Secondary loop: capturing/evolving pets, tower climbs, equipment upgrades, social/guild play, routine dailies; simple yet repetitive.  \n",
      "  • Resources: generally accessible via play/events and long-term planning, though recent reward reductions and RNG frustrations noted.\n",
      "\n",
      "- LiveOps & Retention Systems (q21 - q22)\n",
      "  • Frequent updates, but plagued by stealth changes, reward nerfs, and introduced bugs; perceived monetization-first adjustments erode trust.  \n",
      "  • Some responsiveness to feedback, yet overall update cadence feels destabilizing.\n",
      "\n",
      "- Monetization & Game Economy (q23 - q30)\n",
      "  • Gacha includes pity/visible guarantees, improving perceived safety.  \n",
      "  • Pull prices/costs viewed as high; frequent, expensive banners and top-tier items push spend.  \n",
      "  • Major feature access for F2P is possible, but late-game/PvP competitiveness strongly favors spenders.  \n",
      "  • Spending pressure rated heavy, with “money pit” sentiment and benefit reductions over time.\n",
      "\n",
      "- Anti-P2W Mechanism (q31 - q32)\n",
      "  • Free rewards rated slightly positive; F2P can progress and occasionally earn valuable items over time.  \n",
      "  • Nonetheless, quality/quantity deemed inconsistent, and F2P remains disadvantaged in competitive modes; calls for more generous, event-driven rewards.\n",
      "\n",
      "- IP Integration (q33 - q36)\n",
      "  • Uses Tang Dynasty/Chang’an atmosphere with Shan Hai Jing and Journey to the West motifs; manga tie-in and assorted pop-cultural nods.  \n",
      "  • Depth slightly positive but uneven—some find it rich and evocative, others see awkward mashups, derivative designs, and unnecessary crossovers.\n",
      "\n",
      "- System Requirement (q37 - q40)\n",
      "  • Heavy client footprint with hidden/automatic resource downloads; storage can balloon rapidly.  \n",
      "  • Patching/update experience weak: crashes, stalls, PC version loading issues, network instability during events.\n",
      "\n",
      "- Other Issues (q41)\n",
      "  • PC optimization gaps; frequent bugs/crashes; server instability.  \n",
      "  • Harsh/opaque bans, poor customer service, weak communication, and stealth adjustments.  \n",
      "  • Forced grouping, matchmaking/friction for teams, excessive daily time sinks.  \n",
      "  • Economy/trading complaints, repetitive content, UI/visual polish concerns, intrusive prompts.  \n",
      "  • Account/login hassles, limited social tools, doubts about long-term retention.\n"
     ]
    }
   ],
   "source": [
    "print(general_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b45db380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: one row with game_link and commentary\n",
    "with open(\"output/synthesis_results/general_summary.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"game_link\", \"commentary\"])  # header\n",
    "    writer.writerow([GAME_URL, general_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa195af-9c54-4209-abc0-56d8d2208c2e",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c794781b-636f-4d65-bd14-b3fe20288484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_reviews = list(filter(lambda x: x['review_id'] in [34550869, 34872059], unprocessed_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5884b55-749f-4348-ad83-ca3fb5aece81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c1afa-f18c-4ef6-be29-de7878006214",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = PROMPT_PREFIX + \"\\n\\nINPUT REVIEWS TO ANALYZE (NOTE THAT THERE COULD BE MULTIPLE REVIEWS)\\n\" + json.dumps(test_reviews[0], ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7108dd61-6252-4eaf-8928-d9b6925f9b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda355a-3b0b-48eb-8e4e-e0b5f6bd6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call API\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": test_prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ OpenAI API error in batch {i+1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8c6ee-c5e3-4036-906f-c608c07b7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923d2c1-6601-4080-b548-9f8a70c36902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Strip leading ```json or ``` if present\n",
    "if content.startswith(\"```json\"):\n",
    "    content = re.sub(r\"^```json\\s*\", \"\", content)\n",
    "    content = re.sub(r\"\\s*```$\", \"\", content)\n",
    "elif content.startswith(\"```\"):\n",
    "    content = re.sub(r\"^```\\s*\", \"\", content)\n",
    "    content = re.sub(r\"\\s*```$\", \"\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19caeb-3ac9-43b4-b6aa-5a2966b75961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df1261-7ab6-4368-856b-0ef49a978cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read entire content as a single string\n",
    "with open(\"output/batch_001_raw.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7450e6-216f-4ebe-9823-09bf42217101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5a255-832b-4866-86c1-4d54cd84e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Strip leading ```json or ``` if present\n",
    "if content.startswith(\"```json\"):\n",
    "    content = re.sub(r\"^```json\\s*\", \"\", content)\n",
    "    content = re.sub(r\"\\s*```$\", \"\", content)\n",
    "elif content.startswith(\"```\"):\n",
    "    content = re.sub(r\"^```\\s*\", \"\", content)\n",
    "    content = re.sub(r\"\\s*```$\", \"\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d830d2-d7ec-42ff-b4e0-5147cf0d27c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382cdde-1d2d-4b23-99f8-3d6ddf6e4dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Try full parse first\n",
    "try:\n",
    "    new_results = json.loads(content)\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"⚠️ Full JSON parse failed — attempting to salvage complete objects...\")\n",
    "\n",
    "    # ✅ Extract individual JSON objects inside the top-level array using regex\n",
    "    object_matches = re.findall(r'{.*?}(?=,|\\s*\\])', content, re.DOTALL)\n",
    "    new_results = []\n",
    "\n",
    "    for i, obj_str in enumerate(object_matches):\n",
    "        try:\n",
    "            new_results.append(json.loads(obj_str))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"⚠️ Skipping malformed object #{i+1}\")\n",
    "\n",
    "    print(f\"✅ Salvaged {len(new_results)} valid review(s) from truncated response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82392574-da07-43a2-97b1-be1d46bbe12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a9930-10e9-4407-82d0-06ca9f752cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = json.loads(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
